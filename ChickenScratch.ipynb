{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets: 1728 Replies: 14820\r\n",
      "Vocabulary size: 8594 ,  26889\n"
     ]
    }
   ],
   "source": [
    "from data import *\n",
    "from transformer import *\n",
    "\n",
    "twitterData, twitterTokenizers = loadTransformerData('data/')\n",
    "inputVocabSize = len(twitterTokenizers[0].wordMap)\n",
    "targetVocabSize = len(twitterTokenizers[1].wordMap)\n",
    "print()\n",
    "print(\"Vocabulary size:\", inputVocabSize, \", \", targetVocabSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "conversationData, conversationTokenizers = loadConversationData()\n",
    "inputVocabSize = len(conversationTokenizers[0].wordMap)\n",
    "targetVocabSize = len(conversationTokenizers[1].wordMap)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, tokenizers = loadStableData(100)\n",
    "#\n",
    "# inputVocabSize = tokenizers.pt.get_vocab_size().numpy()\n",
    "# targetVocabSize = tokenizers.en.get_vocab_size().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "numLayers = 4\n",
    "embeddingDepth = 128\n",
    "feedDepth = 512\n",
    "numHeads = 8\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(CustomSchedule(embeddingDepth), beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "model = Transformer(numLayers=numLayers, inputVocabSize=inputVocabSize, targetVocabSize=targetVocabSize, embeddingDepth=embeddingDepth,\n",
    "                    feedDepth=feedDepth, numHeads=numHeads)\n",
    "model.compile(loss=masked_loss, optimizer=optimizer, metrics=[masked_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "464/464 [==============================] - 601s 1s/step - loss: 8.9864 - masked_accuracy: 0.0668\n",
      "Epoch 2/20\n",
      "464/464 [==============================] - 585s 1s/step - loss: 7.2132 - masked_accuracy: 0.0912\n",
      "Epoch 3/20\n",
      "464/464 [==============================] - 586s 1s/step - loss: 6.7358 - masked_accuracy: 0.1229\n",
      "Epoch 4/20\n",
      "464/464 [==============================] - 589s 1s/step - loss: 6.3885 - masked_accuracy: 0.1421\n",
      "Epoch 5/20\n",
      "464/464 [==============================] - 567s 1s/step - loss: 6.0980 - masked_accuracy: 0.1612\n",
      "Epoch 6/20\n",
      "464/464 [==============================] - 550s 1s/step - loss: 5.8448 - masked_accuracy: 0.1791\n",
      "Epoch 7/20\n",
      "464/464 [==============================] - 557s 1s/step - loss: 5.6137 - masked_accuracy: 0.1973\n",
      "Epoch 8/20\n",
      "464/464 [==============================] - 599s 1s/step - loss: 5.3893 - masked_accuracy: 0.2170\n",
      "Epoch 9/20\n",
      "464/464 [==============================] - 606s 1s/step - loss: 5.1850 - masked_accuracy: 0.2356\n",
      "Epoch 10/20\n",
      "464/464 [==============================] - 607s 1s/step - loss: 4.9345 - masked_accuracy: 0.2590\n",
      "Epoch 11/20\n",
      "464/464 [==============================] - 607s 1s/step - loss: 4.6721 - masked_accuracy: 0.2847\n",
      "Epoch 12/20\n",
      "464/464 [==============================] - 599s 1s/step - loss: 4.4299 - masked_accuracy: 0.3091\n",
      "Epoch 13/20\n",
      "464/464 [==============================] - 583s 1s/step - loss: 4.2119 - masked_accuracy: 0.3330\n",
      "Epoch 14/20\n",
      "464/464 [==============================] - 587s 1s/step - loss: 4.0109 - masked_accuracy: 0.3561\n",
      "Epoch 15/20\n",
      "464/464 [==============================] - 586s 1s/step - loss: 3.8280 - masked_accuracy: 0.3785\n",
      "Epoch 16/20\n",
      "464/464 [==============================] - 594s 1s/step - loss: 3.6619 - masked_accuracy: 0.4010\n",
      "Epoch 17/20\n",
      "464/464 [==============================] - 593s 1s/step - loss: 3.5143 - masked_accuracy: 0.4208\n",
      "Epoch 18/20\n",
      "464/464 [==============================] - 668s 1s/step - loss: 3.3842 - masked_accuracy: 0.4398\n",
      "Epoch 19/20\n",
      "464/464 [==============================] - 574s 1s/step - loss: 3.2706 - masked_accuracy: 0.4569\n",
      "Epoch 20/20\n",
      "464/464 [==============================] - 571s 1s/step - loss: 3.1614 - masked_accuracy: 0.4730\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1e028571b80>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit(conversationData, epochs=40)\n",
    "model.fit(twitterData, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "# now = datetime.now()\n",
    "# timeText = now.strftime(\"%y.%m.%d.%H.%M.%S\")\n",
    "# model.export(\"model \" + timeText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "from transformer import Chat\n",
    "chat = Chat(twitterTokenizers, model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey man\n",
      "the best president we never had\n",
      "woah\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'woah'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[40], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m prompt \u001B[38;5;241m=\u001B[39m \u001B[38;5;28minput\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mChat with Chicken: \u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(prompt)\n\u001B[1;32m----> 4\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mchat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(output)\n",
      "Cell \u001B[1;32mIn[33], line 16\u001B[0m, in \u001B[0;36mChat.__call__\u001B[1;34m(self, sentence, maxLength)\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, sentence: \u001B[38;5;28mstr\u001B[39m, maxLength: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m256\u001B[39m):\n\u001B[1;32m---> 16\u001B[0m     tokenized \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minToken\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43msentence\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     17\u001B[0m     sentence \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mconstant(tokenized)\n\u001B[0;32m     19\u001B[0m     start \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mcast(tf\u001B[38;5;241m.\u001B[39mconstant(\u001B[38;5;241m1\u001B[39m)[tf\u001B[38;5;241m.\u001B[39mnewaxis], tf\u001B[38;5;241m.\u001B[39mint64)\n",
      "File \u001B[1;32mD:\\PycharmProjects\\dataMining\\Chicken-Scratch\\process.py:31\u001B[0m, in \u001B[0;36mTokenizer.tokenize\u001B[1;34m(self, text)\u001B[0m\n\u001B[0;32m     28\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit(text)\n\u001B[0;32m     29\u001B[0m wordTokens \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizeSentence(sentence) \u001B[38;5;28;01mfor\u001B[39;00m sentence \u001B[38;5;129;01min\u001B[39;00m text]\n\u001B[1;32m---> 31\u001B[0m sequences \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtranslateTokens\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwordTokens\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m sequences\n",
      "File \u001B[1;32mD:\\PycharmProjects\\dataMining\\Chicken-Scratch\\process.py:40\u001B[0m, in \u001B[0;36mTokenizer.translateTokens\u001B[1;34m(self, tokenized_texts)\u001B[0m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtranslateTokens\u001B[39m(\u001B[38;5;28mself\u001B[39m, tokenized_texts):\n\u001B[1;32m---> 40\u001B[0m     sequences \u001B[38;5;241m=\u001B[39m [[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwordMap[word] \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m text] \u001B[38;5;28;01mfor\u001B[39;00m text \u001B[38;5;129;01min\u001B[39;00m tokenized_texts]\n\u001B[0;32m     41\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m sequences\n",
      "File \u001B[1;32mD:\\PycharmProjects\\dataMining\\Chicken-Scratch\\process.py:40\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtranslateTokens\u001B[39m(\u001B[38;5;28mself\u001B[39m, tokenized_texts):\n\u001B[1;32m---> 40\u001B[0m     sequences \u001B[38;5;241m=\u001B[39m [[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwordMap[word] \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m text] \u001B[38;5;28;01mfor\u001B[39;00m text \u001B[38;5;129;01min\u001B[39;00m tokenized_texts]\n\u001B[0;32m     41\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m sequences\n",
      "File \u001B[1;32mD:\\PycharmProjects\\dataMining\\Chicken-Scratch\\process.py:40\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtranslateTokens\u001B[39m(\u001B[38;5;28mself\u001B[39m, tokenized_texts):\n\u001B[1;32m---> 40\u001B[0m     sequences \u001B[38;5;241m=\u001B[39m [[\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwordMap\u001B[49m\u001B[43m[\u001B[49m\u001B[43mword\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m text] \u001B[38;5;28;01mfor\u001B[39;00m text \u001B[38;5;129;01min\u001B[39;00m tokenized_texts]\n\u001B[0;32m     41\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m sequences\n",
      "\u001B[1;31mKeyError\u001B[0m: 'woah'"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    prompt = input(\"Chat with Chicken: \")\n",
    "    print(prompt)\n",
    "    output = chat(prompt)\n",
    "    print(output)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
